# ==============================================================================
# docker-compose.yml â€” mlsys-marconi
#
# Marconi CPU-based trace repro: follow marconi/artifact_evaluation.md (conda + run_all_experiments.sh).
#
# Services
# --------
#   sglang-server : SGLang inference server (GPU, always starts)
#   bench         : Benchmark / evaluation shell (profile: bench)
#
# Quick reference
# ---------------
#   # Start the inference server
#   docker compose up sglang-server
#
#   # Open an interactive benchmark shell once the server is healthy
#   docker compose run --rm --profile bench bench
#
#   # Override model or tensor parallel size via env
#   MODEL_NAME=... TENSOR_PARALLEL=... docker compose up sglang-server
# ==============================================================================

services:
  # ----------------------------------------------------------------------------
  # SGLang inference server (GPU)
  # ----------------------------------------------------------------------------
  sglang-server:
    build:
      context: .
      dockerfile: Dockerfile
      target: sglang
      args:
        SGLANG_BRANCH: ${SGLANG_BRANCH:-}
        SGLANG_VERSION: ${SGLANG_VERSION:-}
        CUDA_SHORT: ${CUDA_SHORT:-cu124}
    ports:
      - "${SGLANG_PORT:-30000}:30000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/workspace/.cache/huggingface
    volumes:
      - hf-cache:/workspace/.cache/huggingface
      - ./results:/workspace/results
      - ./traces:/workspace/traces
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    shm_size: "32gb"
    ipc: host
    ulimits:
      memlock:
        soft: -1
        hard: -1
    command: >
      python3 -m sglang.launch_server
        --model ${MODEL_NAME:-ai21labs/Jamba-1.5-Mini}
        --host 0.0.0.0
        --port 30000
        --tp ${TENSOR_PARALLEL:-4}
        ${SGLANG_EXTRA_ARGS:-}

  # ----------------------------------------------------------------------------
  # Benchmarking shell (same SGLang image, interactive)
  #
  # Waits for sglang-server to be healthy, then drops you into a bash shell
  # from which you can run bench_serving, trace replays, etc.
  #
  # Usage:
  #   docker compose run --rm bench
  #   # then inside the container:
  #   python3 -m sglang.bench_serving \
  #       --backend sglang --base-url http://sglang-server:30000 \
  #       --dataset-name generated-shared-prefix --num-prompts 100
  # ----------------------------------------------------------------------------
  bench:
    build:
      context: .
      dockerfile: Dockerfile
      target: sglang
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN:-}
      - HF_HOME=/workspace/.cache/huggingface
      - SGLANG_SERVER_URL=http://sglang-server:30000
    volumes:
      - hf-cache:/workspace/.cache/huggingface
      - ./results:/workspace/results
      - ./traces:/workspace/traces
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      sglang-server:
        condition: service_healthy
    profiles:
      - bench
    entrypoint: ["bash"]
    stdin_open: true
    tty: true

volumes:
  hf-cache:
    name: marconi-hf-cache
