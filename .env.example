# ==============================================================================
# .env — mlsys-marconi Docker configuration
#
# Copy to .env and edit:  cp .env.example .env
# Defaults below are set for 4x V100 32GB (128GB total VRAM).
# ==============================================================================

# --- Model --------------------------------------------------------------------
# HuggingFace model ID to serve.  Hybrid (Mamba + Attention) models:
#   ai21labs/Jamba-1.5-Mini          (small; use TP=1 or TP=4)
#   Qwen/Qwen3-Next-80B-A3B-Instruct (~80B; 4x V100 32GB may need --quantization)
#   Other 40B–70B hybrids            (good fit for TP=4 on 4x V100 32GB)
MODEL_NAME=ai21labs/Jamba-1.5-Mini

# Number of GPUs for tensor parallelism (set to 4 for 4x V100 32GB)
TENSOR_PARALLEL=4

# HuggingFace token (required for gated models)
HF_TOKEN=

# Extra arguments passed to sglang.launch_server
# Examples:
#   --disable-radix-cache                      (baseline, no caching)
#   --marconi-evict-policy 2 --marconi-eff-weight 0.5  (when Marconi is implemented)
SGLANG_EXTRA_ARGS=

# --- SGLang build -------------------------------------------------------------
# Install from a specific git branch (e.g. PR #11214 for Mamba radix cache).
# Leave empty to install latest release from PyPI.
SGLANG_BRANCH=

# Or pin a specific PyPI release version (takes priority over SGLANG_BRANCH).
SGLANG_VERSION=

# CUDA short version for flashinfer wheel selection
CUDA_SHORT=cu124

# --- Networking ---------------------------------------------------------------
# Host port to expose SGLang server on
SGLANG_PORT=30000
